# -*- coding: utf-8 -*-
"""SEIR_EoN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bCitMSfYkKQJ7Dvk4KSZygGYVCoC6dzk

# Comparison of SEIR epidemic dynamics resulting from evictions on a household network structure

- We describe SEIR epidemic dynamics when multiple social distancing relaxation methods are enacted on a N=10000 node network structure that is divided into household clusters. There are two types of edges of the network: (1) within household connections and (2) between household connections. Within each household cluster, all nodes are connected (complete network). Between household connections are randomly added between every two individuals of different households (binomial network). The network can be described as the combination of these two types of connections.
- The SEIR epidemic parameters are roughly assumed to be similar to inferred parameters of the current coronavirus pandemic. Specifically, we assume an average incubation period of 5 days, an average infectious period of 8 days, and an R0 of 2.5.
- This code was originally extended out of Joel C. Miller's code for a simple SEIR epidemic simulation. Joel C. Miller's original code can be found here: https://epidemicsonnetworks.readthedocs.io/en/latest/functions/EoN.Gillespie_simple_contagion.html#EoN.Gillespie_simple_contagion

#Import libraries and set directory to plot results
"""


import EoN
import networkx as nx
from collections import defaultdict
import matplotlib.pyplot as plt
import random
import math
import scipy
import numpy as np
import matplotlib.patches as mpatches
import os
import calendar
from os import path
from itertools import product
from datetime import date

random.seed(0)

# Directory to save results ----------------------------------------------------
directory_plots = "/Users/michaellevy/Desktop/"

"""#Changeable parameters of epidemic and network structure"""

# Network parameters -----------------------------------------------------------
N = 100000 # Number of nodes
type_ld_graph_to_use = "binomial" # "binomial" or "geographic_heterogeneity"
probability_ld_edge = 0.0002 # Used if the ld graph type used is "binomial"
sigma = 0.01 # Used if the ld graph type used is "geographic heterogeneity" -- changes width of Gaussian kernel
prob_unfuse = 0.1 # Probability of unfusing each day after "I" found in a house under voluntary fusing

# Eviction parameters ----------------------------------------------------------
frac_to_fuse = 1/200 # Initial fraction to fuse on "evictions_date"
fusings = [ # Format is: list of tuple of format (date_of_eviction, frac_to_fuse)
           (date(2020, 9, 1), 1/200),
           (date(2020, 10, 1), 1/200),
           (date(2020, 11, 1), 1/200),
           (date(2020, 12, 1), 1/200),
           (date(2021, 1, 1), 1/200),
           (date(2021, 2, 1), 1/200),
           (date(2021, 3, 1), 1/200),
           (date(2021, 4, 1), 1/200)]

# Shelter parameters -----------------------------------------------------------
num_shelters = 1
shelter_size = 3
#shelter_fusing = None
shelter_fusings = [# Format is: list of tuple of format (date_of_eviction, frac_to_shelter)
                   (date(2020, 9, 1), 0),
                   (date(2020, 10, 1), 0),
                   (date(2020, 11, 1), 0),
                   (date(2020, 12, 1), 0),
                   (date(2021, 1, 1), 0),
                   (date(2021, 2, 1), 0),
                   (date(2021, 3, 1), 0),
                   (date(2021, 4, 1), 0)]

# Epidemic parameters ----------------------------------------------------------
R_0 = 2.5
R_under_SD = 0.85
R_under_easing = 1.15
rate_E_I = 1 / 5
rate_I_R = 1 / 8
sar = 0.3
ratio_beta_house_to_LD = 2.3
beta_initial_local = np.log(1 - sar) / (-8)
beta_initial_ld = beta_initial_local / ratio_beta_house_to_LD

# Timeline parameters ----------------------------------------------------------
number_of_initial_infected = 20
prev_start_SD = 0.02
sd_date = date(2020, 3, 14)
easing_date = date(2020, 6, 1)
evictions_date = date(2020, 8, 1)
end_date = date(2021, 5, 1)
sd_to_easing = (easing_date - sd_date).days
easing_to_evictions = (evictions_date - easing_date).days
evictions_to_end = (end_date - evictions_date).days

# Second social distancing parameters ------------------------------------------
second_SD_date = date(2020, 11, 15)
evictions_to_second_SD = (second_SD_date - evictions_date).days
second_SD_to_end = (end_date - second_SD_date).days
percent_delete_second_SD_date = 0.35

# Simulation parameters --------------------------------------------------------
number_of_simulations = 1

"""# Creating household network structures
- local: network of household edges. Each household is a complete graph, and each household is isolated from each other. Includes shelters.
- ld: network of long distance edges between individuals from different households. Binomial graph.
- O: network combination of local and ld.
- SD: network of O that deletes some percent of the ld edges.
- expanded_SD: network of SD that fuses a fraction of the isolated households of local.
- add_back_long_SD: network that adds back a fraction of the long distance edges to the SD network.
- add_back_long_expanded_SD: network that adds back a fraction of the long distance edges to the expanded_SD graph.

## local
- Create local network structure based. The network can qualitatively described as isolated connected components, where each connected component is complete.
"""

def create_local():
    # Household size distribution from the 2010 census data --------------------
    total_house = 118092823
    one_house = 31532469
    two_house = 38634080
    three_house = 19038803
    four_house = 15853234
    five_house = 7638191
    six_house = 3106133
    seven_house = 2289913

    house_size_dist = np.array([one_house,two_house,three_house,four_house,five_house,six_house,seven_house])/total_house

    household_sizes = []

    while sum(household_sizes) < N:
        household_sizes.extend(np.random.choice(np.arange(1,8,1), p=house_size_dist, size=1)) 

    if sum(household_sizes) > N:
        diff = sum(household_sizes) - N
        household_sizes[len(household_sizes) - 1] = household_sizes[len(household_sizes) - 1] - diff

    local = nx.Graph()
    curr_node = 0
    for household_size in household_sizes:
        household_graph = nx.complete_graph(household_size)

        node_names = dict()
        for node_dex in list(range(household_graph.number_of_nodes())):
            node_names[node_dex] = curr_node
            curr_node += 1

        household_graph = nx.relabel_nodes(household_graph, node_names)

        local.add_nodes_from(household_graph)
        local.add_edges_from(household_graph.edges())
    
    # Add shelter --------------------------------------------------------------
    for shelter_num in range(num_shelters):
        shelter_graph = nx.complete_graph(shelter_size)
        
        for node_dex in list(range(shelter_graph.number_of_nodes())):
            node_names[node_dex] = curr_node
            curr_node += 1

        shelter_graph = nx.relabel_nodes(shelter_graph, node_names)

        local.add_nodes_from(shelter_graph)
        local.add_edges_from(shelter_graph.edges())

    return local

local = create_local()

"""##ld
Choice one (binomial):
- A binomial graph to represent the long distance connections between households
- For every two nodes of the graph, there is a probability of existing depending on the below parameter, probability_ld_edge.

Choice two (geographic heterogeneity):
- A long-distance connections graph between households (connected components)
- We model distance by having each connected component take a place on a 1D line from 0 to 1. All individuals of the household are assigned this location attribute. The probability of edges existing between individuals of different households depends on this location attribute.
"""

def kernel_func(loc_one, loc_two, sigma):
    prob_from_gaussian_kernel = math.exp(-((np.abs(loc_one - loc_two) / sigma) ** 2))
    return prob_from_gaussian_kernel

def create_ld(type_ld_graph, input_local, probability_ld_edge, sigma):
    if (type_ld_graph == "binomial"):
      local = input_local.copy()
      ave_ld_initial = ((R_0 / (1/rate_I_R)) - ((local.number_of_edges() / local.number_of_nodes()) * beta_initial_local)) / beta_initial_ld
      num_ld_add = (ave_ld_initial * input_local.number_of_nodes()) / 2
      N_choose_two = scipy.special.binom(N, 2)
      probability_ld_edge = num_ld_add / N_choose_two
      ld = nx.fast_gnp_random_graph(N, probability_ld_edge)
    else:
      local = input_local.copy()
      ld = nx.Graph()
      ld.add_nodes_from(local)
      ccs = list((local.subgraph(c) for c in nx.connected_components(local)))
      locations = np.random.uniform(low=0.0, high=1.0, size=len(ccs))
      for cc_one_dex in range(len(ccs)):
        for cc_two_dex in range(cc_one_dex + 1, len(ccs)):
          # Probability of edge from group cc_one_dex and group cc_two_dex -----
          prob_edge = kernel_func(locations[cc_one_dex], locations[cc_two_dex], sigma=sigma)
          # Get all potential edges --------------------------------------------
          potential_edges = list(product(list(ccs[cc_one_dex].nodes()), list(ccs[cc_two_dex].nodes())))
          if (len(potential_edges) != (len(ccs[cc_one_dex].nodes()) * len(ccs[cc_two_dex].nodes()))):
            NameError("Number of potential edges is not correct.")
          # Add edges with some probability ------------------------------------
          for potential_edge in potential_edges:
            edge_exists = (np.random.binomial(n=1, p=prob_edge, size=1)[0] == 1)
            if edge_exists:
              ld.add_edge(potential_edge[0], potential_edge[1])
    return ld

ld = create_ld(type_ld_graph=type_ld_graph_to_use, input_local=local,
               probability_ld_edge=probability_ld_edge, sigma=sigma)

"""## O
- Combine the household connections graph, local, and long distance connections graph, ld, into a single graph, O
- Set the weight for each edge and node for the transitions and transmission parameters with or without heterogeneity
"""

def create_O(local, ld):
    O = nx.Graph()
    O.add_nodes_from(local)
    local_edge_attribute_dict = {edge: beta_initial_local for edge in local.edges()}
    O.add_edges_from(local.edges())
    ld_edge_attribute_dict = {edge: beta_initial_ld for edge in ld.edges()}
    O.add_edges_from(ld.edges())
    if (O.number_of_edges() > (local.number_of_edges() + ld.number_of_edges())):
        raise NameError("O is not correct.")
    node_attribute_dict = {node: 1 for node in O.nodes()}
    edge_attribute_dict = {**local_edge_attribute_dict, **ld_edge_attribute_dict}
    nx.set_node_attributes(O, values=node_attribute_dict, name='expose2infect_weight')
    nx.set_edge_attributes(O, values=edge_attribute_dict, name='transmission_weight')
    return [O, node_attribute_dict, edge_attribute_dict]

O_returned = create_O(local=local, ld=ld)
O = O_returned[0]
node_attribute_dict = O_returned[1]
edge_attribute_dict = O_returned[2]

"""## SD
- Create the SD graph out of the O graph by randomly deleting some fraction of the edges of the long distance connections graph, ld
- Save indexes of edges deleted of ld in the global variable: edges_to_delete
"""

def create_SD(local, ld, node_attribute_dict, edge_attribute_dict, percent_edges_to_delete_under_SD_input):
    SD = nx.Graph()
    SD.add_nodes_from(local)
    SD.add_edges_from(local.edges())
    percent_edges_to_delete_under_SD = percent_edges_to_delete_under_SD_input
    edges_to_delete = random.sample(list(range(ld.number_of_edges())), int(round(ld.number_of_edges() * percent_edges_to_delete_under_SD)))
    SD_ld_edges = list(ld.edges)
    for index in sorted(edges_to_delete, reverse=True):
        del SD_ld_edges[index]
    SD.add_edges_from(SD_ld_edges)
    nx.set_node_attributes(SD, values=node_attribute_dict, name='expose2infect_weight')
    nx.set_edge_attributes(SD, values=edge_attribute_dict, name='transmission_weight')
    if (SD.number_of_edges() > (local.number_of_edges() + ld.number_of_edges() - round(ld.number_of_edges() * percent_edges_to_delete_under_SD))):
        raise NameError("SD is not correct.")
    return [SD, edges_to_delete]

"""### Introduce method to add back long distance edges in the future
- Args: SD, frac_add_back
- Return: added_frac_long_SD
- Note 1: the attribute of each edge that is added back will still be in the edge_attribute_dict, so we do not need to worry about losing the edge attribute.
"""

def add_back_long(SD, frac_add_back, edge_attribute_dict, edges_deleted):
    added_frac_long_SD = SD.copy()
    edges_to_add_back = random.choices(edges_deleted, k=round(len(edges_deleted) * frac_add_back))
    for index in edges_to_add_back:
      added_frac_long_SD.add_edge(list(ld.edges())[index][0], list(ld.edges())[index][1])
    nx.set_edge_attributes(added_frac_long_SD, values=edge_attribute_dict, name='transmission_weight')
    return added_frac_long_SD

"""## fuse
- Choice one: create the expanded SD graph by randomly fusing some fraction of the household clusters into pairs
- Choice two: create the expanded SD graph by isolating persons from their household who are infectious

### Introduce method to expand a quarantine circle (i.e. involuntary fusing)
- Args: local, frac_islands_fuse
- Return: expanded_local
- Description: Allow fusing houses with both symptomatic and non-symptomatic individuals.
"""

def expand_local(input_local, frac_islands_fuse, already_fused, frac_fusing_to_shelter):
    # Get all households -------------------------------------------------------
    local = input_local.copy()
    ccs = list((local.subgraph(c) for c in nx.connected_components(local)))
    
    # If there are no fusions, then all households are listed as able to fuse --
    if already_fused is None:
        pairs = []
        possible_fusions = list(range(len(ccs)))
    else:
        pairs = already_fused
        possible_fusions = list(range(len(ccs)))
        # Eliminate houses already fused ---------------------------------------
        unlisted_pairs = []
        for pair in pairs:
            unlisted_pairs.append(pair[0])
            unlisted_pairs.append(pair[1])
        possible_fusions = list(set(possible_fusions) - set(unlisted_pairs))
        possible_fusions = list(np.sort(possible_fusions))
    # Randomly select households to further expand -----------------------------
    ccs_to_expand = random.sample(possible_fusions, round(len(possible_fusions) * frac_islands_fuse))

    # Create array that describes probability of picking each cc based on the 
    # number of nodes (contacts) of the cc. 
    # Conserves index of ccs list.
    # This array is "parallel" to the ccs_to_expand array ----------------------
    probability_choose = []
    for cc_expand_dex in list(range(len(ccs_to_expand))):
        probability_choose.append(ccs[ccs_to_expand[cc_expand_dex]].number_of_nodes())

    while (len(ccs_to_expand) > 1):
        # Get the index of the island in "ccs_to_expand" and "probability_choose"
        curr_probability_choose = np.divide(probability_choose, sum(probability_choose))
        ccs_to_expand_dex = np.random.choice(a=list(range(len(ccs_to_expand))), size=1, p=curr_probability_choose)
        
        # Get first island, and pop off this element from the probability_choose parallel array
        first_island = ccs_to_expand.pop(ccs_to_expand_dex[0])
        probability_choose.pop(ccs_to_expand_dex[0])
        
        # Get the index of the island in "ccs_to_expand" and "probability_choose"
        curr_probability_choose = np.divide(probability_choose, sum(probability_choose))
        ccs_to_expand_dex = np.random.choice(a=list(range(len(ccs_to_expand))), size=1, p=curr_probability_choose)
        
        # Get second island, and pop off this element from the probability_choose parallel array
        second_island = ccs_to_expand.pop(ccs_to_expand_dex[0])
        probability_choose.pop(ccs_to_expand_dex[0])
        
        pairs.append((first_island, second_island))
    
    # Initialize a dictionary that maps nodes to edge sets created during fusing
    fused_edges_dict = dict()
    
    # Create the modified local graph with fused connected components ----------
    new_local = nx.Graph()
    for pair in pairs:
        island_one = pair[0]
        island_two = pair[1]
        n_nodes = ccs[island_one].number_of_nodes() + ccs[island_two].number_of_nodes()
        
        # Create new complete graph named "temp" that has number of nodes == 
        # number of nodes in island one + number of nodes in island two. Also,
        # "temp" conserves the original node names from the original graph -----
        temp = nx.Graph()
        temp.add_nodes_from(ccs[island_one])
        temp.add_nodes_from(ccs[island_two])
        
        # Create dictionary of the node names from the original island ---------
        node_names = dict()
        for node_dex in list(range(n_nodes)):
            node_names[node_dex] = list(temp.nodes())[node_dex]
        
        # Create the new "fused" connected component ---------------------------
        fused = nx.complete_graph(n_nodes)
        fused = nx.relabel_nodes(fused, node_names)
        
        new_local.add_nodes_from(fused.nodes())
        new_local.add_edges_from(fused.edges())

        # Save the edges that were added in order to fuse the islands ----------
        original_component = nx.compose(ccs[island_one], ccs[island_two])
        added_edges_graph = nx.difference(fused, original_component)
        if fused.number_of_edges() != (added_edges_graph.number_of_edges() + original_component.number_of_edges()):
            NameError("Difference of edges is not correct.")
        
        # Add a key-value entry from each node to the set of edges that should
        # be deleted in the case that the component unfuses --------------------
        for node in added_edges_graph.nodes():
            fused_edges_dict[node] = added_edges_graph.edges()
    
    # Add the ccs that were not chosen to expand -------------------------------
    ccs_not_to_expand = np.setdiff1d(list(range(len(ccs))), ccs_to_expand)
    for cc_num in ccs_not_to_expand:
        new_local.add_nodes_from(ccs[cc_num])
        new_local.add_edges_from(ccs[cc_num].edges())
    
    # Small correction to add in last connected component that may have been
    # chosen to expand, but did not have a pair to expand with -----------------
    if len(ccs_to_expand) == 1:
        last_island = ccs_to_expand[0]
        new_local.add_nodes_from(ccs[last_island])
        new_local.add_edges_from(ccs[last_island].edges())

    # Fuse frac_fusing_to_shelter to the shelter hubs --------------------------
    if frac_fusing_to_shelter is not None:
        # Find all those that are shelters -------------------------------------
        ccs = list((new_local.subgraph(c) for c in nx.connected_components(new_local)))
        shelters = []
        non_shelters = []
        for cc in ccs:
            if (cc.number_of_nodes() > 20):
                shelters.append(cc.copy())
            else:
                shelters.append(cc.copy())
        to_shelter_dexs = random.sample(list(range(len(non_shelters))), round(len(non_shelters) * frac_fusing_to_shelter))
        for to_shelter_dex in to_shelter_dexs:
            for node1 in non_shelters[to_shelter_dex].nodes():
                shelter_dex = random.sample(list(range(len(shelters))), 1)
                for node2 in shelters[shelter_dex].nodes():
                    new_local.add_edge(node1, node2)

                    # TODO fused_edges_dict_add --------------------------------

    return [new_local, fused_edges_dict, pairs]

"""### Introduce method to expand a quarantine circle based on whether there is an infection seeded within the component (voluntary fusing)
- Args: local, infected_nodes, frac_islands_fuse
- Return: expanded_local_selected
- Description: only allow fusing houses with no symptomatic individuals.
- Notes: there are minor inefficiencies in the creation of the dictionary for new edges of fused clusters.
"""

def expand_local_select(input_local, infected_nodes, frac_islands_fuse):
    local = input_local.copy()
    ccs = list((local.subgraph(c) for c in nx.connected_components(local)))

    # Find those connected components with at least one currently infected node
    cc_dex = 0
    ccs_infected = []
    for cc in ccs:
      for node in cc.nodes():
        if node in infected_nodes:
          if cc_dex not in ccs_infected:
            ccs_infected.append(cc_dex)
      cc_dex += 1

    # Find the complement ------------------------------------------------------
    ccs_without_infected = np.setdiff1d(list(range(len(ccs))), ccs_infected)
    
    # Figure out how many from frac_to_fuse should fuse to shelter based on some parameter of fraction
    # of frac_to_fuse ----------------------------------------------------------
    

    # Randomly select some of the complement to expand (fuse) ------------------
    ccs_to_expand = random.sample(list(range(len(ccs_without_infected))), round(len(ccs_without_infected) * frac_islands_fuse))
    ccs_to_expand_save = ccs_to_expand.copy()
    pairs = list()
    while (len(ccs_to_expand) > 1):
        first_island_dex = random.randrange(0, len(ccs_to_expand)) # TODO check why don't need [0] index
        first_island = ccs_to_expand.pop(first_island_dex)
        
        second_island_dex = random.randrange(0, len(ccs_to_expand))
        second_island = ccs_to_expand.pop(second_island_dex)
        
        pairs.append((first_island, second_island))
    
    # Initialize a dictionary that maps nodes to edge sets created during fusing
    fused_edges_dict = dict()

    # Create the modified local graph with fused connected components ----------
    new_local = nx.Graph()
    for pair in pairs:
        island_one = pair[0]
        island_two = pair[1]
        n_nodes = ccs[island_one].number_of_nodes() + ccs[island_two].number_of_nodes()
        
        # Create new complete graph named "temp" that has number of nodes == 
        # number of nodes in island one + number of nodes in island two. Also,
        # "temp" conserves the original node names from the original graph -----
        temp = nx.Graph()
        temp.add_nodes_from(ccs[island_one])
        temp.add_nodes_from(ccs[island_two])
        
        # Create dictionary of the node names from the original island, "temp" -
        node_names = dict()
        for node_dex in list(range(n_nodes)):
            node_names[node_dex] = list(temp.nodes())[node_dex]
        
        # Create the new "fused" connected component ---------------------------
        fused = nx.complete_graph(n_nodes)
        fused = nx.relabel_nodes(fused, node_names)
        
        new_local.add_nodes_from(fused.nodes())
        new_local.add_edges_from(fused.edges())

        # Save the edges that were added in order to fuse the islands ----------
        original_component = nx.compose(ccs[island_one], ccs[island_two])
        added_edges_graph = nx.difference(fused, original_component)
        if fused.number_of_edges() != (added_edges_graph.number_of_edges() + original_component.number_of_edges()):
            NameError("Difference of edges is not correct.")

        # Add a key-value entry from each node to the set of edges that should
        # be deleted in the case that the component unfuses --------------------
        for node in added_edges_graph.nodes():
            fused_edges_dict[node] = added_edges_graph.edges()

    # Add the connected components that have not expanded (fused) --------------
    ccs_not_to_expand = np.setdiff1d(list(range(len(ccs))), ccs_to_expand_save)
    for cc_num in ccs_not_to_expand:
        new_local.add_nodes_from(ccs[cc_num])
        new_local.add_edges_from(ccs[cc_num].edges())
    
    # Small correction to add in last connected component that may have been
    # chosen to expand, but did not have a pair to expand with -----------------
    if len(ccs_to_expand) == 1:
        last_island = ccs_to_expand[0]
        new_local.add_nodes_from(ccs[last_island])
        new_local.add_edges_from(ccs[last_island].edges())
    
    # Add in shelter -----------------------------------------------------------
    return [new_local, fused_edges_dict]

"""### Create expanded quarantine graph out of SD graph
- Create expanded_local graph by randomly doubling up of the local graph
- Assign edge attributes to new edges of expanded_local graph
- Update global dictionary to included attributes for new edges
- Add the new edges to copy of SD graph
- Add node and edge attributes to expanded_SD graph
"""

def create_expanded_SD(local, SD, 
                       node_attribute_dict, edge_attribute_dict,
                       frac_to_fuse,
                       already_fused,
                       frac_fusing_to_shelter):
    expand_local_return = expand_local(local, frac_to_fuse, already_fused, frac_fusing_to_shelter)
    expanded_local = expand_local_return[0]
    fused_edges_nonselect_dict = expand_local_return[1]
    already_fused_returned = expand_local_return[2]
    new_edges = nx.difference(expanded_local, local).edges()
    expanded_edge_attribute_dict = {edge: beta_initial_local for edge in new_edges}
    edge_attribute_dict.update(expanded_edge_attribute_dict)
    expanded_SD = SD.copy()
    expanded_SD.add_edges_from(new_edges)
    nx.set_node_attributes(expanded_SD, values=node_attribute_dict, name='expose2infect_weight')
    nx.set_edge_attributes(expanded_SD, values=edge_attribute_dict, name='transmission_weight')
    return[expanded_SD, edge_attribute_dict, fused_edges_nonselect_dict, already_fused_returned, expanded_local]

"""## unfuse
- Create method to unfuse connected components with some probability if an individual becomes symptomatic
"""

def unfuse_graph(input_graph, symptomatic_nodes, fused_edges_dict, prob_unfuse=0.5):
    graph = input_graph.copy()
    # For each symptomatic individual, there is prob_unfuse probability that the
    # fused connected components they are a part of will unfuse. We find the 
    # edge set that was added during fusing via the fused_edges_dict object ----
    for symptomatic_node in symptomatic_nodes:
        # First, check if it is even part of a fused connected component -------
        if symptomatic_node in fused_edges_dict:
            # Flip a biased coin of p=prob_unfuse to see whether it should unfuse since it
            # is symptomatic
            should_unfuse = np.random.binomial(1, prob_unfuse, 1)
            if (should_unfuse == 1):
                # The following command will not do anything if the edges have
                # already been removed from "graph" --------------------------------
                graph.remove_edges_from(fused_edges_dict[symptomatic_node])
    return graph

"""# Set the parameters of the SEIR epidemic
- Set the spontaneous transition rate parameters and transmission rate parameters for the SEIR simulation
- We base our calculation of beta using the equation of R0 described here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2760158/
"""

H = nx.DiGraph()
H.add_node('S')
H.add_edge('E', 'I', rate = rate_E_I, weight_label='expose2infect_weight')
H.add_edge('I', 'R', rate = rate_I_R)

J = nx.DiGraph()
J.add_edge(('I', 'S'), ('I', 'E'), rate = 1, weight_label='transmission_weight')

"""# Run simulations

## Helper method to run the simulation until a certain prevalence
"""

# Helper method to run simulation until a certain prevalence is reached --------
def run_until_prev(input_net, H, J, initial_conditions,
                   prev_start_SD):
    net = input_net.copy()
    next_step_IC = initial_conditions
    t = None
    S = None
    E = None
    I = None
    R = None
    
    curr_prev = 0
    while (curr_prev < prev_start_SD):
        # Run for one time step ------------------------------------------------
        full_net_one_step = EoN.Gillespie_simple_contagion(net, H, J, next_step_IC, return_statuses, tmax = 1, return_full_data=True)    
        t_one_step = full_net_one_step.t()
        S_one_step = full_net_one_step.S()
        E_one_step = full_net_one_step.summary()[1]['E']
        I_one_step = full_net_one_step.I()
        R_one_step = full_net_one_step.R()

        # Concatenate results of the single time step --------------------------
        if ((t is None) and (S is None) and (E is None) and (I is None) and (R is None)):
            t = t_one_step
            S = S_one_step
            E = E_one_step
            I = I_one_step
            R = R_one_step
        else:
            t = np.concatenate((t, (t_one_step + t[-1])), axis=None)
            S = np.concatenate((S, S_one_step), axis=None)
            E = np.concatenate((E, E_one_step), axis=None)
            I = np.concatenate((I, I_one_step), axis=None)
            R = np.concatenate((R, R_one_step), axis=None)
        
        # Get prevalence -------------------------------------------------------
        curr_prev = (E[-1] + I[-1]) / N

        # Get initial conditions for next step of simulation -------------------
        nodes_one_step_final = full_net_one_step.get_statuses(list(range(N)), t_one_step[-1])
        next_step_IC = defaultdict(lambda: 'S')
        for node in range(N):
            status = nodes_one_step_final[node]
            next_step_IC[node] = status

    # Create complete returnable object of simulation --------------------------
    to_add = list()
    to_add.append(t)
    to_add.append(S)
    to_add.append(E)
    to_add.append(I)
    to_add.append(R)

    full_net = full_net_one_step

    last_time_step_dictionary = full_net.get_statuses(time=full_net.t()[-1])
    
    to_return = [last_time_step_dictionary, to_add, t[-1]]
    
    return to_return

"""## Helper method to run the simulation for the relaxation of social distancing events"""

# Helper method to run relaxation of social distancing -----------------------

# The t_O and t_SD arguments are the arrays that track the time passed so far 
# with the original network structure, and the social distancing strucure, 
# respectively -----------------------------------------------------------------

# The t, S, E, I, and R arguments are understood to the arrays that track the 
# time, susceptibles, exposed, infected and recovereds of the simulation so far,
# before relaxation of social distancing is added ------------------------------

# The unfuse argument refers to whether fused connected components will unfuse
# when there is a sign of symptomatic infection within their cluster. This arg
# will not do anything if there are no fused connected components --------------

# The fused_edges_dict arg is a dictionary from nodes to sets of edges that
# were added during the fusing process. If the argument unfuse==True, then the
# fused_edges_dict argument will be used ---------------------------------------

# The length_sim arg refers to the amount of time the simulation should run ----

def add_relaxation(input_net, H, J, initial_conditions,
                   t, S, E, I, R,
                   unfuse,
                   fused_edges_dict,
                   length_sim):
    net = input_net.copy()
    
    # If unfuse, perhaps tedious, but have not found a better way to do it: 
    # check every day for infectious persons in the graph. ---------------------
    if unfuse:
        next_step_IC = initial_conditions
        save_last_R = 0
        while not ((E[-1] == 0 and I[-1] == 0 and R[-1] == save_last_R)):
            save_last_R = R[-1]
            
            # Run for one time step --------------------------------------------
            full_net_one_step = EoN.Gillespie_simple_contagion(net, H, J, next_step_IC, return_statuses, tmax = 1, return_full_data=True)    
            t_one_step = full_net_one_step.t()
            S_one_step = full_net_one_step.S()
            E_one_step = full_net_one_step.summary()[1]['E']
            I_one_step = full_net_one_step.I()
            R_one_step = full_net_one_step.R()

            # Concatenate results of the single time step ----------------------
            t = np.concatenate((t, (t_one_step + t[-1])), axis=None)
            S = np.concatenate((S, S_one_step), axis=None)
            E = np.concatenate((E, E_one_step), axis=None)
            I = np.concatenate((I, I_one_step), axis=None)
            R = np.concatenate((R, R_one_step), axis=None)

            # Get list of nodes that are infectious at the end of this time step
            # as well as get initial conditions for next step of simulation ----
            nodes_one_step_final = full_net_one_step.get_statuses(list(range(N)), t_one_step[-1])
            next_step_IC = defaultdict(lambda: 'S')
            symptomatic_nodes = []
            for node in range(N):
                status = nodes_one_step_final[node]
                next_step_IC[node] = status
                if status == "I":
                    symptomatic_nodes.append(node)

            # Run unfuse method ------------------------------------------------
            net = unfuse_graph(input_graph=net, 
                               symptomatic_nodes=symptomatic_nodes, 
                               fused_edges_dict=fused_edges_dict, 
                               prob_unfuse=prob_unfuse)

        # Create complete returnable object of simulation ----------------------
        to_add = list()
        to_add.append(t)
        to_add.append(S)
        to_add.append(E)
        to_add.append(I)
        to_add.append(R)

        full_net = full_net_one_step
    else:
        full_net = EoN.Gillespie_simple_contagion(net, H, J, initial_conditions, return_statuses, tmax = length_sim, return_full_data=True)    
        t_net = full_net.t()
        S_net = full_net.S()
        E_net = full_net.summary()[1]['E']
        I_net = full_net.I()
        R_net = full_net.R()

        t = np.concatenate((t, (t_net + t[-1])), axis = None)
        S = np.concatenate((S, S_net), axis=None)
        E = np.concatenate((E, E_net), axis=None)
        I = np.concatenate((I, I_net), axis=None)
        R = np.concatenate((R, R_net), axis=None)

        to_add = list()
        to_add.append(t)
        to_add.append(S)
        to_add.append(E)
        to_add.append(I)
        to_add.append(R)
    
    last_time_step_dictionary = full_net.get_statuses(time=full_net.t()[-1])
    
    to_return = [last_time_step_dictionary, to_add]
    
    return to_return

"""## Loop for simulations
- Run stochastic simulations for each SD_day of intervention. Each simulation will run the SEIR epidemic on the O graph until the SD day, then switch to a scenario of relaxation.
"""

# Randomly initialize infections -----------------------------------------------
IC = defaultdict(lambda: 'S')
for node in range(number_of_initial_infected): 
    IC[node] = 'I'

# Prepare loop for simulations -------------------------------------------------
return_statuses = ('S', 'E', 'I', 'R')
cumul_sims_FSD = []
final_status_FSD = []
cumul_sims_EQ = []
final_status_EQ = []
SD_days = []
cumul_num_houses = []
cumul_num_edges_O = []
cumul_num_ld_edges_O = []
cumul_num_edges_SD = []
cumul_num_ld_edges_SD = []
cumul_num_edges_added_frac_long_SD = []
cumul_num_ld_edges_added_frac_long_SD = []

for iteration in list(range(number_of_simulations)):
    print('Iteration: ' + str(iteration))
    # Create new local graph ---------------------------------------------------
    local = create_local()
    cumul_num_houses.append(nx.number_connected_components(local))

    # Create new ld graph ------------------------------------------------------
    ld = create_ld(type_ld_graph="binomial", input_local=local,
                   probability_ld_edge=probability_ld_edge, sigma=sigma)
    cumul_num_ld_edges_O.append(ld.number_of_edges())

    # Create new O graph -------------------------------------------------------
    O_returned = create_O(local=local, ld=ld)
    O = O_returned[0]
    node_attribute_dict = O_returned[1]
    edge_attribute_dict = O_returned[2]

    # Calculate number of edges to reduce to have R_t == R_under_SD ------------
    ave_ld_SD = ((R_under_SD / (1/rate_I_R)) - ((local.number_of_edges() / local.number_of_nodes()) * beta_initial_local)) / beta_initial_ld
    num_ld_SD = (ave_ld_SD * local.number_of_nodes()) / 2
    percent_ld_to_delete = 1 - (num_ld_SD / ld.number_of_edges())
    cumul_num_ld_edges_SD.append(num_ld_SD)

    # Create new SD graph ------------------------------------------------------
    SD_returned = create_SD(local=local, ld=ld, 
                            node_attribute_dict=node_attribute_dict, 
                            edge_attribute_dict=edge_attribute_dict,
                            percent_edges_to_delete_under_SD_input=percent_ld_to_delete)
    SD = SD_returned[0]
    edges_deleted = SD_returned[1]
    
    # Calculate number of edges to add to have R_t == R_under_easing -----------
    ave_ld_easing = ((R_under_easing / (1/rate_I_R)) - ((local.number_of_edges() / local.number_of_nodes()) * beta_initial_local)) / beta_initial_ld
    num_ld_easing = (ave_ld_easing * local.number_of_nodes()) / 2
    num_ld_edges_to_add_back = num_ld_easing - num_ld_SD
    cumul_num_ld_edges_added_frac_long_SD.append(num_ld_easing)

    # Create an "easing" graph -------------------------------------------------
    N_choose_two = scipy.special.binom(N, 2)
    probability_ld_edge = num_ld_edges_to_add_back / N_choose_two
    new_ld = nx.fast_gnp_random_graph(N, probability_ld_edge)
    added_frac_long_SD = SD.copy()
    added_frac_long_SD.add_edges_from(new_ld.edges())
    new_ld_edge_attribute_dict = {edge: beta_initial_ld for edge in new_ld.edges()}
    edge_attribute_dict = {**edge_attribute_dict, **new_ld_edge_attribute_dict}
    nx.set_edge_attributes(added_frac_long_SD, values=edge_attribute_dict, name='transmission_weight')
    
    #percent_ld_to_add_back = num_ld_edges_to_add_back / ld.number_of_edges()
    #added_frac_long_SD = add_back_long(SD=SD, frac_add_back=percent_ld_to_add_back, 
    #                                   edge_attribute_dict=edge_attribute_dict, 
    #                                   edges_deleted=edges_deleted)
    
    # Save number of edges information -----------------------------------------
    cumul_num_edges_O.append(O.number_of_edges())
    cumul_num_edges_SD.append(SD.number_of_edges())
    cumul_num_edges_added_frac_long_SD.append(added_frac_long_SD.number_of_edges())

    # Create new EQ graph ------------------------------------------------------
    expanded_SD_returned = create_expanded_SD(local=local, SD=added_frac_long_SD, 
                                              node_attribute_dict=node_attribute_dict, 
                                              edge_attribute_dict=edge_attribute_dict,
                                              frac_to_fuse=(frac_to_fuse / 5),
                                              already_fused=None,
                                              frac_fusing_to_shelter=None)
    expanded_SD=expanded_SD_returned[0]
    edge_attribute_dict=expanded_SD_returned[1]
    fused_edges_nonselect_dict=expanded_SD_returned[2]
    already_fused_returned=expanded_SD_returned[3]

    # First, run on the original graph until certain prevalence ----------------
    returned_run_until_prev = run_until_prev(input_net=O, H=H, J=J, initial_conditions=IC,
                                             prev_start_SD=prev_start_SD)
    t_O = returned_run_until_prev[1][0]
    S_O = returned_run_until_prev[1][1]
    E_O = returned_run_until_prev[1][2]
    I_O = returned_run_until_prev[1][3]
    R_O = returned_run_until_prev[1][4]
    nodes_O_final = returned_run_until_prev[0]
    SD_day = returned_run_until_prev[2]
    SD_days.append(SD_day)
    
    # Next, run on the SD graph ------------------------------------------------
    SD_IC = defaultdict(lambda: 'S')
    for node in range(N):
        SD_IC[node] = nodes_O_final[node]
    full_SD = EoN.Gillespie_simple_contagion(SD, H, J, SD_IC, return_statuses, tmax = sd_to_easing, return_full_data=True)
    t_SD = full_SD.t()
    S_SD = full_SD.S()
    E_SD = full_SD.summary()[1]['E']
    I_SD = full_SD.I()
    R_SD = full_SD.R()
    nodes_SD_final = full_SD.get_statuses(list(SD.nodes()), t_SD[-1])
    
    # Next, run on the added_frac_long_SD graph --------------------------------
    expanded_SD_IC = defaultdict(lambda: 'S')
    for node in range(N):
        expanded_SD_IC[node] = nodes_SD_final[node]
    full_expanded_SD = EoN.Gillespie_simple_contagion(added_frac_long_SD, H, J, expanded_SD_IC, return_statuses, tmax=easing_to_evictions, return_full_data=True)
    t_expanded_SD = full_expanded_SD.t()
    S_expanded_SD = full_expanded_SD.S()
    E_expanded_SD = full_expanded_SD.summary()[1]['E']
    I_expanded_SD = full_expanded_SD.I()
    R_expanded_SD = full_expanded_SD.R()
    nodes_expanded_SD_final = full_expanded_SD.get_statuses(list(added_frac_long_SD.nodes()), t_expanded_SD[-1])

    # Combine the time series of the past three network structures (i.e.
    # the epidemic so far) into a single time series ---------------------------
    t = np.concatenate((t_O, (t_SD + t_O[-1])), axis=None)
    S = np.concatenate((S_O, S_SD), axis=None)
    E = np.concatenate((E_O, E_SD), axis=None)
    I = np.concatenate((I_O, I_SD), axis=None)
    R = np.concatenate((R_O, R_SD), axis=None)
    t = np.concatenate((t, (t_expanded_SD + t[-1])), axis=None)
    S = np.concatenate((S, S_expanded_SD), axis=None)
    E = np.concatenate((E, E_expanded_SD), axis=None)
    I = np.concatenate((I, I_expanded_SD), axis=None)
    R = np.concatenate((R, R_expanded_SD), axis=None)

    # Create the initial conditions when relaxation is implemented, as well 
    # as get the list of infecteds at day of relaxation ------------------------
    relax_IC = defaultdict(lambda: 'S')
    infected_nodes = []
    for node in range(N):
        status_node = nodes_expanded_SD_final[node]
        relax_IC[node] = status_node
        if status_node == "I":
           infected_nodes.append(node)

    # Next, finish sim on the continued social distancing graph ----------------
    ## First run until the second SD date --------------------------------------
    finished_FSD = add_relaxation(input_net=added_frac_long_SD, H=H, J=J, initial_conditions=relax_IC,
                                  t=t, S=S, E=E, I=I, R=R,
                                  unfuse=False,
                                  fused_edges_dict=None,
                                  length_sim=evictions_to_second_SD)
    
    relax_SD_2_IC = defaultdict(lambda: 'S')
    for node in range(N):
        status_node = finished_FSD[0][node]
        relax_SD_2_IC[node] = status_node

    ld_FSD = nx.difference(added_frac_long_SD, local)
    edges_to_keep_ld_FSD = random.sample(list(ld_FSD.edges()), math.floor(ld_FSD.number_of_edges() * (1 - percent_delete_second_SD_date)))
    added_frac_long_SD_2 = nx.Graph()
    added_frac_long_SD_2.add_nodes_from(local.nodes())
    added_frac_long_SD_2.add_edges_from(local.edges())
    added_frac_long_SD_2.add_edges_from(edges_to_keep_ld_FSD)
    nx.set_node_attributes(added_frac_long_SD_2, values=node_attribute_dict, name='expose2infect_weight')
    nx.set_edge_attributes(added_frac_long_SD_2, values=edge_attribute_dict, name='transmission_weight')
    
    ## Next, run until the end -------------------------------------------------
    finished_FSD_2 = add_relaxation(input_net=added_frac_long_SD_2, H=H, J=J, initial_conditions=relax_SD_2_IC,
                                  t=finished_FSD[1][0], 
                                  S=finished_FSD[1][1], 
                                  E=finished_FSD[1][2], 
                                  I=finished_FSD[1][3], 
                                  R=finished_FSD[1][4],
                                  unfuse=False,
                                  fused_edges_dict=None,
                                  length_sim=second_SD_to_end)
    final_status_FSD.append(finished_FSD_2[0])
    cumul_sims_FSD.append(finished_FSD_2[1])

    # Next, finish sim on the expanded quarantine graph ------------------------
    t_EQ = t.copy()
    S_EQ = S.copy()
    E_EQ = E.copy()
    I_EQ = I.copy()
    R_EQ = R.copy()
    
    curr_initial_conditions = relax_IC.copy()
    past_fusing_date = evictions_date
    # For all elements of the vector "fusings" ---------------------------------
    for fusing_tuple_dex in range(len(fusings)):
        fusing_tuple = fusings[fusing_tuple_dex]
        shelter_fusing_tuple = shelter_fusings[fusing_tuple_dex]
        print(fusing_tuple)
        # Get eviction date and percent fused on this eviction date ------------
        fusing_date = fusing_tuple[0]
        num_days_run = (fusing_date - past_fusing_date).days

        # Run simulation for rest of month -------------------------------------
        finished_EQ = add_relaxation(input_net=expanded_SD, H=H, J=J, initial_conditions=curr_initial_conditions,
                                     t=t_EQ, S=S_EQ, E=E_EQ, I=I_EQ, R=R_EQ,
                                     unfuse=False,
                                     fused_edges_dict=fused_edges_nonselect_dict.copy(),
                                     length_sim=num_days_run)
        # Update the results up to curr_month ----------------------------------
        t_EQ = finished_EQ[1][0].copy()
        S_EQ = finished_EQ[1][1].copy()
        E_EQ = finished_EQ[1][2].copy()
        I_EQ = finished_EQ[1][3].copy()
        R_EQ = finished_EQ[1][4].copy()
        # Graph for new fusions in the next time step --------------------------
        percent_fused = fusing_tuple[1]
        percent_fused_to_shelter = shelter_fusing_tuple[1]
        expanded_SD_returned = create_expanded_SD(local=local, SD=expanded_SD, 
                                                  node_attribute_dict=node_attribute_dict.copy(), 
                                                  edge_attribute_dict=edge_attribute_dict.copy(),
                                                  frac_to_fuse=percent_fused,
                                                  already_fused=already_fused_returned,
                                                  frac_fusing_to_shelter=percent_fused_to_shelter)
        expanded_SD=expanded_SD_returned[0].copy()
        edge_attribute_dict=expanded_SD_returned[1].copy()
        fused_edges_nonselect_dict=expanded_SD_returned[2].copy()
        already_fused_returned=expanded_SD_returned[3].copy()
        expanded_local=expanded_SD_returned[4].copy()

        # Check if social distancing has been enacted a second time ------------
        if second_SD_date >= past_fusing_date and second_SD_date <= fusing_date:
            ld_at_second_SD_date = nx.difference(expanded_SD, expanded_local)
            edges_to_keep = random.sample(list(ld_at_second_SD_date.edges()), math.floor(ld_at_second_SD_date.number_of_edges() * (1 - percent_delete_second_SD_date)))
            expanded_SD = nx.Graph()
            expanded_SD.add_nodes_from(expanded_local.nodes())
            expanded_SD.add_edges_from(expanded_local.edges())
            expanded_SD.add_edges_from(edges_to_keep)
            nx.set_node_attributes(expanded_SD, values=node_attribute_dict, name='expose2infect_weight')
            nx.set_edge_attributes(expanded_SD, values=edge_attribute_dict, name='transmission_weight')
    
        # Update curr_initial_conditions and past_eviction_date variables ------
        curr_initial_conditions = finished_EQ[0].copy()
        past_fusing_date = fusing_date
    # Run simulation for rest of month -----------------------------------------
    num_days_run = (end_date - fusing_date).days
    finished_EQ = add_relaxation(input_net=expanded_SD, H=H, J=J, initial_conditions=curr_initial_conditions,
                                 t=t_EQ, S=S_EQ, E=E_EQ, I=I_EQ, R=R_EQ,
                                 unfuse=False,
                                 fused_edges_dict=fused_edges_nonselect_dict.copy(),
                                 length_sim=num_days_run)
    # Add the results after while loop finished --------------------------------
    final_status_EQ.append(finished_EQ[0])
    cumul_sims_EQ.append(finished_EQ[1])

"""# Output .csvs of adjacency matrix by household --------------------------------
ccs = list((local.subgraph(c) for c in nx.connected_components(local)))

counter = 0
for graph in [O, SD, added_frac_long_SD]:
    rows = []
    for cc1_dex in range(len(ccs)):
        row = []
        for cc2_dex in range((cc1_dex + 1), len(ccs)):
            cell = 0
            for node1 in list(ccs[cc1_dex].nodes()):
                for node2 in list(ccs[cc2_dex].nodes()):
                    if graph.has_edge(node1, node2):
                        cell = 1
            row.append(cell)
        rows.append(row)
    
    rows.reverse()

    if counter == 0:
        graph_type = "O"
    elif counter == 1:
        graph_type = "SD"
    elif counter == 2:
        graph_type = "added_frac_long_SD"
    with open("/content/results/" + graph_type + ".csv", 'w') as out_f:
        for l in range(len(rows)):
            for l2 in range(len(rows[l])):
              if l2 != (len(rows[l]) - 1):
                  out_f.write(str(rows[l][l2]))
                  out_f.write(",")
              else:
                  out_f.write(str(rows[l][l2]))
                  out_f.write("\n")
    print(str(counter))
    print(graph_type)
    counter += 1

# Final statuses ---------------------------------------------------------------
ccs = list((local.subgraph(c) for c in nx.connected_components(local)))
rows = []
for cc in ccs:
    count_infect_or_recover_O = 0
    count_infect_or_recover_SD = 0
    count_infect_or_recover_added_frac_long_SD = 0
    for node in cc.nodes():
        O_status = nodes_O_final[node]
        SD_status = nodes_SD_final[node]
        added_frac_long_SD_status = nodes_expanded_SD_final[node]
        if O_status == "R" or O_status == "I":
            count_infect_or_recover_O += 1
        if SD_status == "R" or SD_status == "I":
            count_infect_or_recover_SD += 1
        if added_frac_long_SD_status == "R" or added_frac_long_SD_status == "I":
            count_infect_or_recover_added_frac_long_SD += 1
    row = [count_infect_or_recover_O, count_infect_or_recover_SD, count_infect_or_recover_added_frac_long_SD]
    rows.append(row)

with open("/content/results/final_statuses.csv", 'w') as out_f:
    for l in range(len(rows)):
        out_f.write(str(rows[l][0]))
        out_f.write(",")
        out_f.write(str(rows[l][1]))
        out_f.write(",")
        out_f.write(str(rows[l][2]))
        out_f.write("\n")

# Plot simulation results
- Plot comparison of simulations of the aforementioned scenarios of relaxed social distancing
"""

if not (path.exists(directory_plots)):
  os.mkdir(directory_plots)

os.chdir(directory_plots)
num_batch = len(os.listdir())
new_batch_folder_name = directory_plots + "batch" + str(num_batch + 1) + "/"
os.mkdir(new_batch_folder_name)
os.mkdir(new_batch_folder_name + "csvs/")

# Find min, median, and max final sizes depending on relaxation type -----------
def find_statistics_final_size(sims):
    final_sizes = []
    for sim_num in list(range(len(sims))):
        sim_final_size = sims[sim_num][4][-1]
        final_sizes.append(sim_final_size)
    median_fs = np.median(final_sizes)
    max_fs = max(final_sizes)
    min_fs = min(final_sizes)
    to_return = [round(((min_fs / N) * 100), 3), round(((median_fs / N) * 100), 3), round(((max_fs / N) * 100), 3), final_sizes]
    return to_return

# Find min, median, and max paired differences of final sizes ------------------
def find_paired_difference_fs(sims1, sims2):
    differences = []
    for sim_num in list(range(len(sims1))):
        difference = (sims1[sim_num][4][-1] - sims2[sim_num][4][-1])
        differences.append(difference)
    median_fs = np.median(differences)
    max_fs = max(differences)
    min_fs = min(differences)
    to_return = [round(((min_fs / N) * 100), 3), round(((median_fs / N) * 100), 3), round(((max_fs / N) * 100), 3), differences]
    return to_return

statistics_final_size_FSD = find_statistics_final_size(cumul_sims_FSD)
min_final_size_FSD = statistics_final_size_FSD[0]
median_final_size_FSD = statistics_final_size_FSD[1]
max_final_size_FSD = statistics_final_size_FSD[2]
final_sizes_FSD = statistics_final_size_FSD[3]
statistics_final_size_EQ = find_statistics_final_size(cumul_sims_EQ)
min_final_size_EQ = statistics_final_size_EQ[0]
median_final_size_EQ = statistics_final_size_EQ[1]
max_final_size_EQ = statistics_final_size_EQ[2]
final_sizes_EQ = statistics_final_size_EQ[3]
paired_difference_fs = find_paired_difference_fs(cumul_sims_EQ, cumul_sims_FSD)
min_paired_difference = paired_difference_fs[0]
median_paired_difference = paired_difference_fs[1]
max_paired_difference = paired_difference_fs[2]
paired_differences = paired_difference_fs[3]

# Write csvs of final size and paired difference statistics --------------------
np.savetxt(new_batch_folder_name + 'final_sizes_FSD.csv', final_sizes_FSD, delimiter=",")
np.savetxt(new_batch_folder_name + 'final_sizes_EQ.csv', final_sizes_EQ, delimiter=",")
np.savetxt(new_batch_folder_name + 'paired_differences.csv', paired_differences, delimiter=",")

# Prepare legend text ----------------------------------------------------------
black_patch = mpatches.Patch(color='black', label='SD continued; FS %: ' + str(median_final_size_FSD) + ' [' + str(min_final_size_FSD) + ', ' + str(max_final_size_FSD) + ']')
red_patch = mpatches.Patch(color='red', label='involuntary fusing; FS %: ' + str(median_final_size_EQ) + ' [' + str(min_final_size_EQ) + ', ' + str(max_final_size_EQ) + ']')
diff_patch = mpatches.Patch(color='white', label='paired difference FS %: ' + str(median_paired_difference) + ' [' + str(min_paired_difference) + ', ' + str(max_paired_difference) + ']')
param_patch = mpatches.Patch(color='white', label='Network parameters -------------------- \nNum. houses: ' + str(round(np.median(cumul_num_houses))) + ' [' + str(min(cumul_num_houses)) + ', ' + str(max(cumul_num_houses)) + ']' +  
                             '\nNum. edges init: ' +  str(round(np.median(cumul_num_edges_O))) + ' [' + str(min(cumul_num_edges_O)) + ', ' + str(max(cumul_num_edges_O)) + ']' +
                             '\nRt under SD: ' + str(R_under_SD)  +  '\nNum. edges SD: ' +  str(round(np.median(cumul_num_edges_SD))) + ' [' + str(min(cumul_num_edges_SD)) + ', ' + str(max(cumul_num_edges_SD)) + ']' +
                             '\nRt under easing: ' + str(R_under_easing)  + '\nNum. edges easing: ' +  str(round(np.median(cumul_num_edges_added_frac_long_SD))) + ' [' + str(min(cumul_num_edges_added_frac_long_SD)) + ', ' + str(max(cumul_num_edges_added_frac_long_SD)) + ']' +
                             '\n% households fused initial: ' + str(round((100 * frac_to_fuse), 2)) +
                             '\nNum LD @ initial: ' + str(round(np.median(cumul_num_ld_edges_O))) + ' [' + str(round(min(cumul_num_ld_edges_O))) + ', ' + str(round(max(cumul_num_ld_edges_O))) + ']' + 
                             '\nNum LD @ SD: ' + str(round(np.median(cumul_num_ld_edges_SD))) + ' [' + str(round(min(cumul_num_ld_edges_SD))) + ', ' + str(round(max(cumul_num_ld_edges_SD))) + ']' + 
                             '\nNum LD @ easing: ' + str(round(np.median(cumul_num_ld_edges_added_frac_long_SD))) + ' [' + str(round(min(cumul_num_ld_edges_added_frac_long_SD))) + ', ' + str(round(max(cumul_num_ld_edges_added_frac_long_SD))) + ']' + 
                             '\nSEIR epidemic parameters ------------ \nR0: ' + str(R_0) + '; E to I: ' + str(1 / rate_E_I) + ' days; I to R: ' + str(1 / rate_I_R) + ' days')

# Find min, median and max prevalence after a certain cutoff_day ---------------
def prev_after_day(sims, cutoff_day):
    prevs = []
    for sim_num in list(range(len(sims))):
        if np.where(sims[sim_num][0] >= cutoff_day)[0].size != 0:
            prev_dex = np.where(sims[sim_num][0] >= cutoff_day)[0][0]
            prev = sims[sim_num][4][prev_dex]
            prevs.append(prev)
    if len(prevs) > 0:
        min_prev = min(prevs)
        median_prev = np.median(prevs)
        max_prev = max(prevs)
        to_return = [round(((min_prev / N) * 100), 3), round(((median_prev / N) * 100), 3), round(((max_prev / N) * 100), 3), prevs]
    else:
        to_return = [0, 0, 0, prevs]
    return to_return
statistics_prev_FSD = prev_after_day(cumul_sims_FSD, SD_days[0])
min_prev_FSD = statistics_prev_FSD[0]
median_prev_FSD = statistics_prev_FSD[1]
max_prev_FSD = statistics_prev_FSD[2]
prevs_FSD = statistics_prev_FSD[3]
statistics_prev_easing = prev_after_day(cumul_sims_FSD, SD_days[0] + sd_to_easing)
min_prev_easing = statistics_prev_easing[0]
median_prev_easing = statistics_prev_easing[1]
max_prev_easing = statistics_prev_easing[2]
prevs_easing = statistics_prev_easing[3]
statistics_prev_evictions = prev_after_day(cumul_sims_FSD, SD_days[0] + sd_to_easing + easing_to_evictions)
min_prev_evictions = statistics_prev_evictions[0]
median_prev_evictions = statistics_prev_evictions[1]
max_prev_evictions = statistics_prev_evictions[2]
prevs_evictions = statistics_prev_evictions[3]
prev_patch = mpatches.Patch(color='white', label='% prev. @ SD intervention: ' + str(median_prev_FSD) + ' [' + str(min_prev_FSD) + ', ' + str(max_prev_FSD) + ']')
prev_patch_two = mpatches.Patch(color='white', label='% prev. @ easing: ' + str(median_prev_easing) + ' [' + str(min_prev_easing) + ', ' + str(max_prev_easing) + ']')
prev_patch_three = mpatches.Patch(color='white', label='% prev. @ evictions: ' + str(median_prev_evictions) + ' [' + str(min_prev_evictions) + ', ' + str(max_prev_evictions) + ']')

# Write csvs of prevalences at different points of the epidemic ----------------
np.savetxt(new_batch_folder_name + 'prevs_FSD.csv', prevs_FSD, delimiter=",")
np.savetxt(new_batch_folder_name + 'prevs_easing.csv', prevs_easing, delimiter=",")
np.savetxt(new_batch_folder_name + 'prevs_evictions.csv', prevs_evictions, delimiter=",")

# Save metadata and statistics -------------------------------------------------
to_write = []
to_write.append('SD continued; FS %: ' + str(median_final_size_FSD) + ' [' + str(min_final_size_FSD) + ', ' + str(max_final_size_FSD) + ']')
to_write.append('involuntary fusing; FS %: ' + str(median_final_size_EQ) + ' [' + str(min_final_size_EQ) + ', ' + str(max_final_size_EQ) + ']')
to_write.append('paired difference FS %: ' + str(median_paired_difference) + ' [' + str(min_paired_difference) + ', ' + str(max_paired_difference) + ']')
to_write.append('Network parameters -------------------- \nNum. houses: ' + str(round(np.median(cumul_num_houses))) + ' [' + str(min(cumul_num_houses)) + ', ' + str(max(cumul_num_houses)) + ']' +  
                '\nNum. edges init: ' +  str(round(np.median(cumul_num_edges_O))) + ' [' + str(min(cumul_num_edges_O)) + ', ' + str(max(cumul_num_edges_O)) + ']' +
                '\nRt under SD: ' + str(R_under_SD)  +  ' Num. edges SD: ' +  str(round(np.median(cumul_num_edges_SD))) + ' [' + str(min(cumul_num_edges_SD)) + ', ' + str(max(cumul_num_edges_SD)) + ']' +
                '\nRt under easing: ' + str(R_under_easing)  +  ' Num. edges easing: ' +  str(round(np.median(cumul_num_edges_added_frac_long_SD))) + ' [' + str(min(cumul_num_edges_added_frac_long_SD)) + ', ' + str(max(cumul_num_edges_added_frac_long_SD)) + ']' +
                '\n% households fused initial: ' + str(round((100 * frac_to_fuse), 2)) +
                '\nNum LD @ initial: ' + str(round(np.median(cumul_num_ld_edges_O))) + ' [' + str(round(min(cumul_num_ld_edges_O))) + ', ' + str(round(max(cumul_num_ld_edges_O))) + ']' + 
                '\nNum LD @ SD: ' + str(round(np.median(cumul_num_ld_edges_SD))) + ' [' + str(round(min(cumul_num_ld_edges_SD))) + ', ' + str(round(max(cumul_num_ld_edges_SD))) + ']' + 
                '\nNum LD @ easing: ' + str(round(np.median(cumul_num_ld_edges_added_frac_long_SD))) + ' [' + str(round(min(cumul_num_ld_edges_added_frac_long_SD))) + ', ' + str(round(max(cumul_num_ld_edges_added_frac_long_SD))) + ']' + 
                '\nSEIR epidemic parameters ------------ \nR0: ' + str(R_0) + '; E to I: ' + str(1 / rate_E_I) + ' days; I to R: ' + str(1 / rate_I_R) + ' days')
to_write.append('% prev. @ SD intervention: ' + str(median_prev_FSD) + ' [' + str(min_prev_FSD) + ', ' + str(max_prev_FSD) + ']')
to_write.append('% prev. @ easing: ' + str(median_prev_easing) + ' [' + str(min_prev_easing) + ', ' + str(max_prev_easing) + ']')
to_write.append('% prev. @ evictions: ' + str(median_prev_evictions) + ' [' + str(min_prev_evictions) + ', ' + str(max_prev_evictions) + ']')
to_write.append('shelter_size: ' + str(shelter_size))
to_write.append('num_shelters: ' + str(num_shelters))
to_write.append('second_SD_date: ' + str(second_SD_date))
to_write.append('percent_delete_second_SD_date: ' + str(percent_delete_second_SD_date))
for fusing_tuple_dex in range(len(fusings)):
    fusing_tuple = fusings[fusing_tuple_dex]
    shelter_fusing_tuple = shelter_fusings[fusing_tuple_dex]
    to_write.append('fusing_tuple: ' + str(fusing_tuple))
    to_write.append('shelter_fusing_tuple: ' + str(shelter_fusing_tuple))
f=open(new_batch_folder_name + 'metadata.txt','w')
for ele in to_write:
    f.write(ele+'\n')
f.close()

SD_day_dex = 0
for sim_num in range(number_of_simulations):
    # Plot the stochastic simulations together ---------------------------------
    plt.xlim(0, 500)
    plt.ylim(0, 2)
    plt.ylabel('% N infected')
    plt.xlabel('t')
    plt.title('Prevalence vs. time')
    ax = plt.subplot(111)
    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.9, box.height])
    plt.legend(handles=[black_patch, red_patch, diff_patch, param_patch, prev_patch, prev_patch_two, prev_patch_three], prop={'size': 6}, loc='upper left', bbox_to_anchor=(1, 0.8))
    plt.plot(cumul_sims_FSD[sim_num][0], 100 * ((cumul_sims_FSD[sim_num][2] + cumul_sims_FSD[sim_num][3]) / N), label='network', color="black", linewidth=0.1)

    if (np.where(cumul_sims_EQ[sim_num][0] >= (SD_days[SD_day_dex] + sd_to_easing + easing_to_evictions))[0].size != 0):
        first_EQ = np.where(cumul_sims_EQ[sim_num][0] >= (SD_days[SD_day_dex] + sd_to_easing + easing_to_evictions))[0][0]
        plt.plot(cumul_sims_EQ[sim_num][0][first_EQ:], 100 * ((cumul_sims_EQ[sim_num][2][first_EQ:] + cumul_sims_EQ[sim_num][3][first_EQ:]) / N), label='network', color="red", linewidth=0.1)
    
        # Write csvs ---------------------------------------------------------------
        to_save_csv = np.vstack((cumul_sims_FSD[sim_num][0],
                       100 * ((cumul_sims_FSD[sim_num][2] + cumul_sims_FSD[sim_num][3]) / N)))
        to_save_csv = np.vstack((cumul_sims_EQ[sim_num][0][first_EQ:],
                       100 * ((cumul_sims_EQ[sim_num][2][first_EQ:] + cumul_sims_EQ[sim_num][3][first_EQ:]) / N)))
        np.savetxt(new_batch_folder_name + 'csvs/' + str(sim_num) + "_FSD.csv", to_save_csv, delimiter=",")
        np.savetxt(new_batch_folder_name + 'csvs/' + str(sim_num) + "_EQ.csv", to_save_csv, delimiter=",")

    if (np.mod(sim_num + 1, number_of_simulations) == 0):
        plt.axvline(x = SD_days[SD_day_dex])
        plt.axvline(x = (SD_days[SD_day_dex] + sd_to_easing), linestyle='--')
        plt.axvline(x = (SD_days[SD_day_dex] + sd_to_easing + easing_to_evictions), linestyle='--')
        plt.axvline(x = (SD_days[SD_day_dex] + sd_to_easing + easing_to_evictions + evictions_to_second_SD))
        plt.tight_layout(rect=[0,0,1,1])
        file_name = new_batch_folder_name + str(sim_num) + '.png'
        plt.savefig(file_name, dpi=1000, bbox_inches='tight')
        plt.clf()
        SD_day_dex += 1